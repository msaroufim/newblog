---
layout: default
title: Automatic Differentiation
---

# Automatic differentiation

Differentiation shows up everywhere, the backprop algorithm, equations of motion and pretty much every single field or algorithm that needs to quantify a rate of change.

While everyone knows how important derivatives are, less know of the different techniques to solve differentiation. We'll go through all the major techniques for differentiation and spend most of our time talking about automatic differentiation which represents the best tradeoff among our various options.



## The different ways to differentiate

Differentiation is about quantifying the rate of change of a system. 

Picture of what differentiation is about


### Symbolic differentiation

Symbolic differentiation works very much like a compiler. You have some sort of mathematical expression and you can simplify it via various substitution rules. 

For example let's restrict our attention to 3 rules

1. Sum rule: \\( \frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) \\)
2. Constant rule: \\(\frac{d}{dx} C = 0 \\) if \\(C \\) is a constant
3. Derivatives of powers rule: if \\(f(x) = x^r \\) then \\(f'(x) = rx^{r-1} \\)

Let's find the derivative of \\(\frac{d}{dx} x^3 + 3 \\)

By the sum rule we get \\( \frac{d}{dx} x^3 + \frac{d}{dx} 3 \\)

By the constant rule this reduces to \\(\frac{d}{dx} x^3 \\)

By the derivatives of powers rule this reduces to \\(2x^2 \\)


Humans would use a reference sheet of all the rules to derive a simpler expression and much the same way a computer program would have a table of all the rules and apply them to simplify the expression. Oviously there's a lot of other rules we could add to make this work. One unfortunate thing about symbolic differentiation

This works fine for the most part but when the expression is complex, unrolling it can create an expression that's \\(O(2^n) \\) longer than the original expression.


### Numerical differentiation

Numerical differentiation is about seeing how much a function varies if we take tiny steps

\\( \frac{\partial f(x)}{\partial x_i} = \frac{f(x + h e_i) - f(x)}{\epsilon} \\)

where \\(\epsilon \\) a positive small step size and \\(e_i \\) is a basis vector (a direction along one of the coordinate axes)

Suppose we have some function

 \\(f = \frac{dx}{dt} + x = 1 \\)

sadsadsa

### Automatic differentiation

 Automatic differentiation is a combination between a numerical and symbolic algorithm.

 Since this is probably the only technique you haven't studied ad nauseum in the past there's a few prerequisite ideas we need to go through
 1. Taylor series expansion
 2. Dual numbers
 3. Computational graphs

To cover two algorithms: forward mode and reverse mode Automatic differentiation


#### Taylor series expansion
The Taylor series of an arbitrary function \\(f(x) \\) at a point in its domain \\(a \\) is

\\( f(a) = \sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} (x-a)^{n} = f(a)+\frac {f'(a)}{1!} (x-a)+ \frac{f''(a)}{2!} (x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+ \cdots \\) 

It's a powerful technique that lets us write any function that's infinitely differentiable as as infinite series. It has numerous applications including function approximation.

<!-- As an example the taylor series of \\(\cos(x)=  1 - \frac{x^2}{2} + \frac{x^4}{4} - \frac{x^6}{6!}  \\) -->



Let's write the Taylor series of \\(f(a + h) \\) where \\(h \\) is a tiny positive number.

\\(f(a + h) = f(a) + hf'(a) + \frac{f''(a)}{2!}h^2 + \frac{f''(a)}{2!}h^2 + \dots \\)

If \\(h \approx x \\) and \\(h > 0 \\) then \\(h^2, h^3 \cdots \approx 0 \\) which means we can simplify the above expression to

\\(f(a+ h) = f(a) + hf'(a) \\)

Moving things around we get back the famous numerical derivative equation we we showed in the previous section.

\\(f'(a) = \lim_{h \to 0}\frac{f(a+h) - f(h)}{h} \\)

However this expression has a major flaw. Since we can't really divide by 0, \\(h \\) will be non zero which means all the higher order terms, i.e: the terms that are a function of \\(h^2, h^3 \\) will all be non zero. 

The numerical derivative always has an associated error which can be mitigated via the correct choice of \\(h \\) but it'll always be there. This problem is not shared by symbolic differentiation which is slow but exact. 

Automatic differentiation is an alternate approach which is fast and exact.


#### Dual numbers
A dual number is a number \\(z  \\)  s.t


\\(z = a + b \epsilon \\) where \\(\epsilon^2 = 0 \\) and \\(a, b \in \mathbb{R}  \\)

Dual numbers look close to complex numbers \\(\mathbb{C} \\) since complex numbers would instead have the condition of \\( \epsilon^2 = -1 \\). Let's do some basic dual number arithmetic to see what their properties are by looking at two dual numbers \\(z_1 = a + b \epsilon \\) and \\(z_2 = c + d \epsilon \\)

\\(z_1 + z_2 = (a + b \epsilon) + (c + d \epsilon) = (a + c) + (b + d)\epsilon \\) 

\\(z_1 * z_2 = (a + b \epsilon) * (c + d\epsilon) = ac + ad \epsilon + bc \epsilon + bd \epsilon^2 = ac + (ad + bc) \epsilon   \\)

Feel free to work out \\(z_1 / z_2 \\)




Dual numbers have a very interesting property once we plug them into a Taylor series expansion. Instead of evaluating a function at a point \\(a \\) where \\(\\ a \in \mathbb{R} \\) we evaluate it at a dual number \\(a + b \epsilon \\). The first step of the derivation follows from our derivation of the Taylor series of \\(f(a+h) \\) and the second step follows form the property of dual numbers \\(\epsilon^2 = 0 \\)

 $$\begin{eqnarray} 
f(a + b \epsilon) &=& f(a) + \frac{f'(a)}{1!}b\epsilon + \frac{f''(a)}{2!}(b\epsilon)^2 + \frac{f^3(a)}{3!}(b\epsilon)^3 + \cdots \\\\\\
&=&  f(a) + \frac{f'(a)}{1!}b\epsilon + 0 + 0 + \cdots    \\\\\\
& =&  f(a) + f'(a)b\epsilon            \\\\\\
\end{eqnarray}$$


The final expression holds both \\(f(a) \\) and \\(f'(a) \\).

As an example

\\(f(x) = x^2 + 1 \\)

We'd like to find the derivative of \\(f \\) at \\( x = 2  \\). We can do this by turning \\(x \\) into a dual number and evaluate

\\(f(2 + \epsilon) = (2 + \epsilon)^2 + 1 = 2^2 + 2*2 \epsilon + \epsilon^2 + 1 = 5 + 4 \epsilon \\)

And we can indeed check that \\(f(2) = 5 \\) and \\(f'(2) = 4) (it looks like it's supposed to be 5 I'm confused)

Automatic differentiation is exact because at no point did we assume any constraints on the implementation such as a small \\(h \\), we're also not dealing with mathematical limits which are only true at infinity or in other works not true on a computer. Instead we work with an alternate representation in the form of the dual numbers. This is a recurring theme in all my work, some representations naturally express more than others.


#### Computational graph

 We can encode an arbitrary numerical function using a computational graph. (Libraries like Tensorflow and Pytorch do this too)

 $$\begin{eqnarray} 
y &=& 1+1   \\\\\\
&=& 2
\end{eqnarray}$$

## Generalizing AD to any function
The discussion above may have given you the impression that Automatic Differentiation only works on continuous mathematical functions but in fact it also works for any function in a programming language including ```for``` loops ```if``` conditions etc.. This generalization is called [Differentiable programming](https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html) so in this section we'll look into what differentiable programming means exactly and see it working in ```Julia``` via the [Zygote.jl](https://github.com/FluxML/Zygote.jl) package.
```julia

# julia functions are written like math functions
julia> f(x) = 5x + 3

# evaluate f and 
julia> f(10), f'(10)

(53, 5)
```

```
a+b
```


I can't remember the name of the awesome Julia AD tutorial

https://github.com/MikeInnes/diff-zoo

Concepts I need to cover
* Wengert List
* Forward vs reverse mode AD


```julia
struct Dual{T<:Real} <: Real
  x::T
  ϵ::T
end

Dual(1, 2)
```

## Convergence of numerical optimization and Machine Learning: Neural ODEs

## References
* Neural ODE
    * Adaptive computation
    * Constant memory backprop
* https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795
    * Resnet layer looks like solution to eulers equation
* Perturbation confusion and referential transparency
* AUtomatic differentiation in Machine Learning: a survey
* [Simon's Tech Blog](http://simonstechblog.blogspot.com/2011/11/dual-number.html)
* https://blog.jle.im/entry/purely-functional-typed-models-1.html


* [Functional Pearls: Probabilistic Functional Programming in Haskell](https://web.engr.oregonstate.edu/~erwig/papers/PFP_JFP06.pdf)

How far can we push this idea of typed neural networks as functions? Turns out we can also model probabilistic programs in this manner. Probabilistic programs have built in APIs to deal with probabilistic distributions and operations on top of them. The reason you should care is for the most part observations in the real world are noisy so when you make a sensor measurement for e.g you know both the measurement of the sensor and you have some idea from the manufacturer about the acceptable error rate so instead of dealing with an observation as a real value input it's better to deal with it as a distribution - this idea shows up a ton in SLAM algorithms which allow robots to localize themselves in space while mapping it.

We can say stuff like this

```haskell
-- throw a six sided die
die = uniform [1..6]

-- clap if you play dnd
die = uniform [1..20]

-- what is the prob of getting at least two 6 if you throw 2 die
((>=2) . length. filter (==6) ?? dice 4)

-- distributions can be composed in two ways
-- cartesian composition -> take the result of two die
-- monadic composition -> take the result of the first die and influence the roll of the second die
```

## Talks
* [GOTO 2018 • Machine Learning: Alchemy for the Modern Computer Scientist • Erik Meijer](https://www.youtube.com/watch?v=Rs0uRQJdIcg&list=WL&index=7&t=149s)
* [React 2014 : Erik Meijer - What does it mean to be Reactive?](https://www.youtube.com/watch?v=sTSQlYX5DU0&list=WL&index=6&t=135s)
* [Category Theory, The essence of interface-based design - Erik Meijer](https://www.youtube.com/watch?v=JMP6gI5mLHc&list=WL&index=5&t=132s)
* [Erik Meijer: Functional Programming](https://www.youtube.com/watch?v=z0N1aZ6SnBk&list=WL&index=4&t=132s)
* [The Probability Monad](https://www.youtube.com/watch?v=qZ4O-1VYv4c&list=WL&index=3&t=0s)
* [Eric J. Ma - An Attempt At Demystifying Bayesian Deep Learning](https://www.youtube.com/watch?v=s0S6HFdPtlA&list=WL&index=2&t=635s)