---
layout: default
title: Automatic Differentiation
---

# Automatic differentiation

Differentiation shows up everywhere, the backprop algorithm, equations of motion and pretty much every single field or algorithm that needs to quantify a rate of change.

While everyone knows how important derivatives are, less know of the different techniques to solve differentiation. We'll go through all the major techniques for differentiation and spend most of our time talking about automatic differentiation which represents the best tradeoff among our various options.


## The different ways to differentiate

Differentiation is about quantifying the rate of change of a system. 

Picture of what differentiation is about


### Symbolic differentiation

Symbolic differentiation works very much like a compiler. You have some sort of mathematical expression and you can simplify it via various substitution rules. 

For example let's restrict our attention to 3 rules

1. Sum rule: \\( \frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) \\)
2. Constant rule: \\(\frac{d}{dx} C = 0 \\) if \\(C \\) is a constant
3. Derivatives of powers rule: if \\(f(x) = x^r \\) then \\(f'(x) = rx^{r-1} \\)

Let's find the derivative of \\(\frac{d}{dx} x^3 + 3 \\)

By the sum rule we get \\( \frac{d}{dx} x^3 + \frac{d}{dx} 3 \\)

By the constant rule this reduces to \\(\frac{d}{dx} x^3 \\)

By the derivatives of powers rule this reduces to \\(2x^2 \\)


Humans would use a reference sheet of all the rules to derive a simpler expression and much the same way a computer program would have a table of all the rules and apply them to simplify the expression. Oviously there's a lot of other rules we could add to make this work. One unfortunate thing about symbolic differentiation

This works fine for the most part but when the expression is complex, unrolling it can create an expression that's \\(O(2^n) \\) longer than the original expression.


### Numerical differentiation

Numerical differentiation is about seeing how much a function varies if we take tiny steps

\\( \frac{\partial f(x)}{\partial x_i} = \frac{f(x + h e_i) - f(x)}{\epsilon} \\)

where \\(\epsilon \\) a positive small step size and \\(e_i \\) is a basis vector (a direction along one of the coordinate axes)

Suppose we have some function

 \\(f = \frac{dx}{dt} + x = 1 \\)

sadsadsa

### Automatic differentiation

 Automatic differentiation is a combination between a numerical and symbolic algorithm.

 Since this is probably the only technique you haven't studied ad nauseum in the past there's a few prerequisite ideas we need to go through
 1. Taylor series expansion
 2. Dual numbers
 3. Computational graphs

To cover two algorithms: forward mode and reverse mode Automatic differentiation


#### Taylor series expansion
The Taylor series of an arbitrary function \\(f(x) \\) at a point in its domain \\(a \\) is

\\( f(a) = \sum_{n=0} ^ {\infty} \frac {f^{(n)}(a)}{n!} (x-a)^{n} = f(a)+\frac {f'(a)}{1!} (x-a)+ \frac{f''(a)}{2!} (x-a)^2+\frac{f'''(a)}{3!}(x-a)^3+ \cdots \\) 

It's a powerful technique that lets us write any function that's infinitely differentiable as as infinite series. It has numerous applications including function approximation.

<!-- As an example the taylor series of \\(\cos(x)=  1 - \frac{x^2}{2} + \frac{x^4}{4} - \frac{x^6}{6!}  \\) -->



Let's write the Taylor series of \\(f(a + h) \\) where \\(h \\) is a tiny positive number.

\\(f(a + h) = f(a) + hf'(a) + \frac{f''(a)}{2!}h^2 + \frac{f''(a)}{2!}h^2 + \dots \\)

If \\(h \approx x \\) and \\(h > 0 \\) then \\(h^2, h^3 \cdots \approx 0 \\) which means we can simplify the above expression to

\\(f(a+ h) = f(a) + hf'(a) \\)

Moving things around we get back the famous numerical derivative equation we we showed in the previous section.

\\(f'(a) = \lim_{h \to 0}\frac{f(a+h) - f(h)}{h} \\)

However this expression has a major flaw. Since we can't really divide by 0, \\(h \\) will be non zero which means all the higher order terms, i.e: the terms that are a function of \\(h^2, h^3 \\) will all be non zero. 

The numerical derivative always has an associated error which can be mitigated via the correct choice of \\(h \\) but it'll always be there. This problem is not shared by symbolic differentiation which is slow but exact. 

Automatic differentiation is an alternate approach which is fast and exact.


#### Dual numbers
A dual number is a number \\(z  \\)  s.t


\\(z = a + b \epsilon \\) where \\(\epsilon^2 = 0 \\) and \\(a, b \in \mathbb{R}  \\)

Dual numbers look close to complex numbers \\(\mathbb{C} \\) since complex numbers would instead have the condition of \\( \epsilon^2 = -1 \\). Let's do some basic dual number arithmetic to see what their properties are by looking at two dual numbers \\(z_1 = a + b \epsilon \\) and \\(z_2 = c + d \epsilon \\)

\\(z_1 + z_2 = (a + b \epsilon) + (c + d \epsilon) = (a + c) + (b + d)\epsilon \\) 

\\(z_1 * z_2 = (a + b \epsilon) * (c + d\epsilon) = ac + ad \epsilon + bc \epsilon + bd \epsilon^2 = ac + (ad + bc) \epsilon   \\)

Feel free to work out \\(z_1 / z_2 \\)




Dual numbers have a very interesting property once we plug them into a Taylor series expansion. Instead of evaluating a function at a point \\(a \\) where \\(\\ a \in \mathbb{R} \\) we evaluate it at a dual number \\(a + b \epsilon \\). The first step of the derivation follows from our derivation of the Taylor series of \\(f(a+h) \\) and the second step follows form the property of dual numbers \\(\epsilon^2 = 0 \\)

 $$\begin{eqnarray} 
f(a + b \epsilon) &=& f(a) + \frac{f'(a)}{1!}b\epsilon + \frac{f''(a)}{2!}(b\epsilon)^2 + \frac{f^3(a)}{3!}(b\epsilon)^3 + \cdots \\\\\\
&=&  f(a) + \frac{f'(a)}{1!}b\epsilon + 0 + 0 + \cdots    \\\\\\
& =&  f(a) + f'(a)b\epsilon            \\\\\\
\end{eqnarray}$$


The final expression holds both \\(f(a) \\) and \\(f'(a) \\).

As an example

\\(f(x) = x^2 + 1 \\)

We'd like to find the derivative of \\(f \\) at \\( x = 2  \\). We can do this by turning \\(x \\) into a dual number and evaluate

\\(f(2 + \epsilon) = (2 + \epsilon)^2 + 1 = 2^2 + 2*2 \epsilon + \epsilon^2 + 1 = 5 + 4 \epsilon \\)

And we can indeed check that \\(f(2) = 5 \\) and \\(f'(2) = 4) (it looks like it's supposed to be 5 I'm confused)

Automatic differentiation is exact because at no point did we assume any constraints on the implementation such as a small \\(h \\), we're also not dealing with mathematical limits which are only true at infinity or in other works not true on a computer. Instead we work with an alternate representation in the form of the dual numbers. This is a recurring theme in all my work, some representations naturally express more than others.


#### Computational graph

 We can encode an arbitrary numerical function using a computational graph. (Libraries like Tensorflow and Pytorch do this too)

 $$\begin{eqnarray} 
y &=& 1+1   \\\\\\
&=& 2
\end{eqnarray}$$

## Generalizing AD to any function
The discussion above may have given you the impression that Automatic Differentiation only works on continuous mathematical functions but in fact it also works for any function in a programming language including ```for``` loops ```if``` conditions etc.. This generalization is called [Differentiable programming](https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html) so in this section we'll look into what differentiable programming means exactly and see it working in ```Julia``` via the [Zygote.jl](https://github.com/FluxML/Zygote.jl) package.
```julia

# julia functions are written like math functions
julia> f(x) = 5x + 3

# evaluate f and 
julia> f(10), f'(10)

(53, 5)
```

```
a+b
```


Awesome Julia AD tutorial

https://github.com/MikeInnes/diff-zoo

Concepts I need to cover
* Wengert List
* Forward vs reverse mode AD


```julia
struct Dual{T<:Real} <: Real
  x::T
  ϵ::T
end

Dual(1, 2)
```


## References

* Perturbation confusion and referential transparency
* AUtomatic differentiation in Machine Learning: a survey
* https://blog.jle.im/entry/purely-functional-typed-models-1.html
* * [Simon's Tech Blog](http://simonstechblog.blogspot.com/2011/11/dual-number.html)
* https://blog.jle.im/entry/purely-functional-typed-models-1.html

* [Functional Pearls: Probabilistic Functional Programming in Haskell](https://web.engr.oregonstate.edu/~erwig/papers/PFP_JFP06.pdf)
* [The simple essence of automatic differentiaton](https://arxiv.org/abs/1804.00746)
* [Backprop as Functor: A compositional perspective on supervised learning](https://arxiv.org/abs/1711.10455)

How far can we push this idea of typed neural networks as functions? Turns out we can also model probabilistic programs in this manner. Probabilistic programs have built in APIs to deal with probabilistic distributions and operations on top of them. The reason you should care is for the most part observations in the real world are noisy so when you make a sensor measurement for e.g you know both the measurement of the sensor and you have some idea from the manufacturer about the acceptable error rate so instead of dealing with an observation as a real value input it's better to deal with it as a distribution - this idea shows up a ton in SLAM algorithms which allow robots to localize themselves in space while mapping it.


We can say stuff like this.

```haskell

-- Definition
type Probability = Double
newtype Dist a = Dist {probabilities :: [(a, Probability)]}

-- throw a six sided die
die = uniform [1..6]

-- clap if you play dnd
die = uniform [1..20]

-- what is the prob of getting at least two 6 if you throw 2 die
-- ?? is just an operator that lets us extract values from a probability distribution
((>=2) . length. filter (==6) ?? dice 4)

-- distributions can be composed in two ways
-- cartesian composition -> take the result of two die
-- monadic composition -> take the result of the first die and influence the roll of the second die
```

* [The Probability Monad](https://www.youtube.com/watch?v=qZ4O-1VYv4c&list=WL&index=3&t=0s)

Talk summarizing main ideas from the above paper if you prefer talks. Probability distributions are monads which means we get a free DSL in Haskell to do probabilistic programming by only making sure that distributions adhere to the monad interface. Some details.

```haskell
-- create a distribution
pure :: a -> Dist a

-- map a function over a distribution (this is a functor) 
-- "lift" a function from a context where it operates on values to one where it operates on distributions
fmap :: (a -> b) -> (Dist a -> Dist b)

-- 2 interpretations: Can either sample a nesting of distributions or we can sample the numerous outer distributions (need to explain this a bit more)
-- Distributions of distributions are popular in applications like Gaussian processes or in SLAM but often the DSL we use to work with them is wonky
join:: Dist (Dist a) -> Dist a
```

More cool stuff

```haskell
-- Markov decision process in Haskell
data MDP m s a r = 
  MDP { step:: s -> a -> m (r, s) }

type Policy s a = s -> a

apply :: MDP m s a r
      -> Policy s a
      -> Markov m s r
```

* [Practical Probabilistic Programming with Monads](http://mlg.eng.cam.ac.uk/pub/pdf/SciGhaGor15.pdf)
  * Still need to double check what free monads are http://www.haskellforall.com/2012/06/you-could-have-invented-free-monads.html

* [A novel representation of lists and its applications to the function "reverse"](https://www.cs.tufts.edu/~nr/cs257/archive/john-hughes/lists.pdf) shows you in detail how to turn any function into a list




If you're looking for more Haskell/FunctionalProgramming references you should skim
* [Learn you a Haskell](http://learnyouahaskell.com/) which is great as a first introduction
* Google "Haskell (the thing you're interested in)" since books only go so far some blogs which were particularly illuminating for me were
* [Justin Le's blog](https://blog.jle.im/entry/purely-functional-typed-models-1.html) who formulates many physical and numerical problems in Haskell
* [Graphical linear algebra](https://graphicallinearalgebra.net/) not a Haskell reference but a category reference that discusses how to formulate linear algebra using ideas from category theory
* [Functional Programming in Scala](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653)


## Talks
* [GOTO 2018 • Machine Learning: Alchemy for the Modern Computer Scientist • Erik Meijer](https://www.youtube.com/watch?v=Rs0uRQJdIcg&list=WL&index=7&t=149s)
* [React 2014 : Erik Meijer - What does it mean to be Reactive?](https://www.youtube.com/watch?v=sTSQlYX5DU0&list=WL&index=6&t=135s)
* [Category Theory, The essence of interface-based design - Erik Meijer](https://www.youtube.com/watch?v=JMP6gI5mLHc&list=WL&index=5&t=132s)
  * Computer scientists don't use math enough, Lamport and Meijer say use more math
  * Most math notation is bad and doesn't typecheck, formalism is weird, need to introduce a lot of math before the problems can even be stated, utility of math is not obvious until way later - mechanical operations with matrices are not informative, they become clearer once we understand that a matrix is a linear transformation and that we can compose linear operations with some rules
  * Category theory is about proofs without regards to contents of objects. Programming involves interfaces which are guaranteed without regards to the specifics. We implicitly do category theory when we use design patterns, they're naturally abstract
  * Curry Howard Isomorphism: a type is a theorem and a program is a proof
  * A Category is a programming language a language has types and a morphism is a static method
  * Most objects in programming are abstract so we need an interface and algebraic properties of the required implementation
  * ADjunctions let us do function currying via the exponential operator

* [The Probability Monad](https://www.youtube.com/watch?v=qZ4O-1VYv4c&list=WL&index=3&t=0s)
